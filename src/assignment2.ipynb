{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 DSC 102 2020 WI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this assignment we will conduct data engineering for the Amazon dataset. The extracted features will be used for your next assignment, where you train a model (or models) to predict user ratings for a product.\n",
    "\n",
    "We will be using Apache Spark for this assignment. The default Spark API will be DataFrame, as it is now the recommended choice over the RDD API. That being said, please feel free to switch back to the RDD API if you see it as a better fit for the task. We provide you an option to request RDD format to start with. Also you can switch between DataFrame and RDD in your solution. \n",
    "\n",
    "You will be conducting the tasks on AWS EMR. You will first spawn a smaller cluster for development and then switch to a deployment cluster for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset description\n",
    "You are expected to extract features from three tables, their schema and descriptions are listed below:\n",
    "```\n",
    "1. product\n",
    "    |-- asin: string, the product id, e.g., 'B00I8HVV6E'\n",
    "    |-- salesRank: map, a map between category and sales rank, e.g., {'Home &amp; Kitchen': 796318}\n",
    "    |    |-- key: string, category, e.g., 'Home &amp; Kitchen'\n",
    "    |    |-- value: integer, rank, e.g., 796318\n",
    "    |-- categories: array, list of list of categories, e.g., [['Home & Kitchen', 'Artwork']]\n",
    "    |    |-- element: array, list of categories, e.g., ['Home & Kitchen', 'Artwork']\n",
    "    |    |    |-- element: string, category, e.g., 'Home & Kitchen'\n",
    "    |-- title: string, title of product, e.g., 'Intelligent Design Cotton Canvas'\n",
    "    |-- price: float, price of product, e.g., 27.9\n",
    "    |-- related: map, related information, e.g., {'also_viewed': ['B00I8HW0UK']}\n",
    "    |    |-- key: string, the attribute name of the information, e.g., 'also_viewed'\n",
    "    |    |-- value: array, array of product ids, e.g., ['B00I8HW0UK']\n",
    "    |    |    |-- element: string product id , e.g., 'B00I8HW0UK'\n",
    "2. product_processed\n",
    "    |-- asin: string, same as above\n",
    "    |-- title: string, the imputed title column, e.g., 'Intelligent Design Cotton Canvas'\n",
    "    |-- category: string, the extracted category column, e.g., 'Home & Kitchen'\n",
    "3. review\n",
    "    |-- reviewerID: string, the review id, e.g., 'A1MIP8H7G33SHC'\n",
    "    |-- asin: string, the product id, e.g., 'B00I8HVV6E'\n",
    "    |-- overall: float, the rating associated with the review, e.g., 5.0\n",
    "```\n",
    "\n",
    "The ```review``` table will be useful for extracting the rating information for each product in Task 1. We will be working primarily with ```product``` table throughout Task 1-4. ```product_processed``` is used for Task 5-6.\n",
    "\n",
    "Refer to https://spark.apache.org/docs/latest/api/python/pyspark.sql.html for API guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task summary\n",
    "You will be asked to complete six tasks in total. In each task you will need to implement a function ```task_i()```. The function signatures are fixed. Each function will take in several inputs and conduct the desired transformations. At the end of each task, you will be asked to extract several statistical properties (mean, variance, etc.) from the transformed data. You will need to programmatically put these properties in a python dictionary named ```res```, the schema of which is also given.\n",
    "\n",
    "Each of the tasks will be tested in unit. It means each function you write will be tested in isolation from the rest of code you write. We will award partial points even if other parts failed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conventions\n",
    "### Result format\n",
    "Each task comes with a pre-defined schema for the output results. The result must be stored as python native dictionary and must contain all the keys and nested structures.\n",
    "\n",
    "For example the following schema:\n",
    "```\n",
    "res\n",
    " | -- count_total: int -- count of total rows of the entire table after your operations\n",
    " | -- mean_price: float -- mean value of column price\n",
    "```\n",
    "The desired python code for composing up the dictionary would be like:\n",
    "\n",
    "```python\n",
    "data = ... # Your transformed data\n",
    "res = {\n",
    "    'count_total': None,\n",
    "    'mean_price': None\n",
    "} # Skeleton given for the result\n",
    "res['count_total'] = data.count() # Do not hard-code the value!\n",
    "res['mean_price'] = data.select(F.avg(F.col('price'))) # Do not hard-code the value!\n",
    "```\n",
    "### Dealing with ```null``` values\n",
    "The input tables contain empty values, ```null``` values and dangling references. You do not to deal with empty values and dangling reference unless instructed. For ```null``` values we will follow the common practice in SQL world. Unless instructed otherwise, you need to ignore all the ```null``` value entries when calculating statistics such as count, mean and variance. Of course, do not ignore ```null``` when you are explicitly asked to count the number of ```null``` entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "You will **not** submit this notebook. Instead, you need to put your implementation of ```task_1``` to ```task_6```, along with all the dependencies you imported, in the file co-located with this notebook: ```assignment2.py```. Then rename the file to ```<your pid>_assignment2.py```. For instance, if your pid is ```a45333444```, your file will be named ```a45333444_assignment2.py```.\n",
    "\n",
    "You need to make sure your script runs under the deployment environment (```emr-launch -n 4 -d```), otherwise you may lose points.\n",
    "\n",
    "TBD: upload the py file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the following parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T21:20:49.222369Z",
     "start_time": "2019-12-10T21:20:49.217352Z"
    }
   },
   "outputs": [],
   "source": [
    "PID = '' # your pid, for instance: 'a43223333'\n",
    "INPUT_FORMAT = 'dataframe' # choose a format of your input data, valid options: 'dataframe', 'rdd'\n",
    "DEPLOY = False # Is it deployment phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T21:20:49.975093Z",
     "start_time": "2019-12-10T21:20:49.641920Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from math import isclose\n",
    "from utilities import SEED\n",
    "from utilities import PA2Test\n",
    "from utilities import PA2Data\n",
    "import time\n",
    "if INPUT_FORMAT == 'dataframe':\n",
    "    import pyspark.ml as M\n",
    "    import pyspark.sql.functions as F\n",
    "    import pyspark.sql.types as T\n",
    "elif INPUT_FORMAT == 'rdd':\n",
    "    import pyspark.mllib as M\n",
    "# Boiler plates\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--py-files utilities.py,assignment2.py \\\n",
    "--master yarn \\\n",
    "--deploy-mode client \\\n",
    "--conf spark.memory.fraction=0.8 \\\n",
    "--conf spark.dynamicAllocation.enabled=false \\\n",
    "--conf spark.sql.crossJoin.enabled=true \\\n",
    "pyspark-shell'\n",
    "class args:\n",
    "    data_root = 's3://dsc102-pa2-public/dataset'\n",
    "    review_filename = 'user_reviews_train.csv'\n",
    "    product_filename = 'metadata_header.csv'\n",
    "    product_processed_filename = 'product_processed.csv'\n",
    "    output_root = 's3://{}-pa2/test_results'.format(PID)\n",
    "    test_results_root = 's3://dsc102-pa2-public/test_results'\n",
    "    pid = PID\n",
    "review_path = os.path.join(args.data_root, args.review_filename)\n",
    "product_path = os.path.join(args.data_root, args.product_filename)\n",
    "product_processed_path = os.path.join(args.data_root, args.product_processed_filename)\n",
    "\n",
    "begin = time.time()\n",
    "\n",
    "spark = SparkSession.builder.appName(args.pid).getOrCreate()\n",
    "url = spark.conf.get(\"spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES\")\n",
    "print(\"Connect to Spark UI: {}\".format(url))\n",
    "\n",
    "path_dict = {\n",
    "    'review': review_path,\n",
    "    'product': product_path,\n",
    "    'product_processed': product_processed_path\n",
    "}\n",
    "\n",
    "tests = PA2Test(spark, args.test_results_root)\n",
    "\n",
    "data_io = PA2Data(spark, path_dict, args.output_root, deploy=DEPLOY)\n",
    "\n",
    "data_dict, count_dict = data_io.load_all(input_format=INPUT_FORMAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T22:16:01.126726Z",
     "start_time": "2019-12-09T22:16:01.116480Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import your own dependencies\n",
    "\n",
    "\n",
    "\n",
    "#-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (WIP) Task0: warm up \n",
    "This task is provided for you to get familiar with Spark API. This task won't be graded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T00:34:58.603154Z",
     "start_time": "2019-12-13T00:34:58.598308Z"
    }
   },
   "outputs": [],
   "source": [
    "def task_0():\n",
    "    res = None\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Task1: mean and count of ratings \n",
    "First you will aggregate and extract some information from the user review table. We want to know for each product, what are the mean rating and the number of ratings it received.\n",
    "\n",
    "Your task is to implement the function below. \n",
    "1. For each product ID ```asin``` in ```product_data```, fetch the mean value of ratings. The ratings are stored in the column ```overall``` of ```review_data```, with product ID referencing to the former table. Store the mean value in a new column named ```meanRating``` in table ```product_data```.\n",
    "\n",
    "1. Similarly, put the count of ratings in a new column named ```countRating```.\n",
    "\n",
    "1. You need to conduct the above operations, then extract some statistics out of the generated columns. You need to put the statistics in a python dictionary named ```res```. The description and schema of it are as follows:\n",
    "    ```\n",
    "    res\n",
    "     | -- count_total: int -- count of total rows of the entire table after your operations\n",
    "     | -- mean_meanRating: float -- mean value of column meanRating\n",
    "     | -- variance_meanRating: float -- variance of meanRating\n",
    "     | -- numNulls_meanRating: int -- count of null-value entries of meanRating\n",
    "     | -- mean_countRating: float -- mean value of countRating\n",
    "     | -- variance_countRating: float -- variance of countRating\n",
    "     | -- numNulls_countRating: int -- count of null-value entries of countRating\n",
    "     \n",
    "    ```\n",
    "If for a product ID, there is not a single reference in ```review```, meaning it was never reviewed, you should put ```null``` in both ```meanRating``` and ```countRating```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T21:30:44.034299Z",
     "start_time": "2019-12-10T21:30:44.012787Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load -s task_1 assignment2.py\n",
    "def task_1(data_io, review_data, product_data):\n",
    "    # -----------------------------Column names--------------------------------\n",
    "    # Inputs:\n",
    "    asin_column = 'asin'\n",
    "    overall_column = 'overall'\n",
    "    # Outputs:\n",
    "    mean_rating_column = 'meanRating'\n",
    "    count_rating_column = 'countRating'\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Your implementation begins------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Put results in res dict --------------------------\n",
    "    # Calculate the values programmaticly. Do not change the keys and do not\n",
    "    # hard-code values in the dict. Your submission will be evaluated with\n",
    "    # different inputs.\n",
    "    # Modify the values of the following dictionary accordingly.\n",
    "    res = {\n",
    "        'count_total': None,\n",
    "        'mean_meanRating': None,\n",
    "        'variance_meanRating': None,\n",
    "        'numNulls_meanRating': None,\n",
    "        'mean_countRating': None,\n",
    "        'variance_countRating': None,\n",
    "        'numNulls_countRating': None\n",
    "    }\n",
    "    # Modify res:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----------------------------- Do not change -----------------------------\n",
    "    data_io.save(res, 'task_1')\n",
    "    return res\n",
    "    # -------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T22:19:04.214179Z",
     "start_time": "2019-12-09T22:18:39.293699Z"
    },
    "deletable": false,
    "editable": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = task_1(data_io, data_dict['review'], data_dict['product'][['asin']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "# Task 2: flattening ```categories``` and ```salesRank```\n",
    "Implement a function ```task_2()``` to conduct the following operations:\n",
    "\n",
    "1. For the ```product``` table, each item in column ```categories``` contains an array of arrays of hierarchical catetories. The schema is ```ArrayType(ArrayType(StringType))```. We are only going to use the most general category, which is the first element of the nested array: ```array[0][0]```. Create a new column named as ```category```. And for each row, put the ```array[0][0]``` of column ```categories``` in ```category```. You should skip those ```null``` entries in ```categories``` and put a ```null``` also in ```categories```. Also put a ```null``` if ```categories``` value is not ```null```, but the array is empty.\n",
    "\n",
    "1. On the other hand, each value in column ```salesRank``` is a dictionary with a single ```(category, rank)``` pair. Your task is to retrieve this key-value pair and put them in two columns, respectively. Put the category in a new column named ```bestSalesCategory``` and the rank in ```bestSalesRank```. You should put ```null``` in these new columns if the original entry in ```salesRank``` was ```null```. Note this ```bestSalesCategory``` may or may not be identical to ```category```.\n",
    "\n",
    "1. You need to conduct the above operations, then extract some statistics out of the generated columns. You need to put the statistics in a python dictionary named ```res```. The description and schema of it are as follows:\n",
    "    ```\n",
    "    res\n",
    "     | -- count_total: int -- count of total rows of the entire table\n",
    "     | -- mean_bestSalesRank: float -- mean value of *bestSalesRank*, excluding null-value entries\n",
    "     | -- variance_bestSalesRank: float -- variance of *bestSalesRank*, excluding null-value entries\n",
    "     | -- numNulls_category: int -- count of null-value entries of *category*\n",
    "     | -- countDistinct_category: int -- count of all distinct values of *category*, excluding null\n",
    "     | -- numNulls_bestSalesCategory: int -- count of null-value entries of *bestSalesCategory*\n",
    "     | -- countDistinct_bestSalesCategory: int -- count of distinct values of *bestSalesCategory*, excluding null\n",
    "    ```\n",
    "\n",
    "Hint: use ```DataFrame.withColumn()``` to apply operations on column and add the result as a new column. To drop a column, use ```DataFrame.drop()```, to rename one, use ```DataFrame.withColumnRenamed()```.\n",
    "\n",
    "References: https://spark.apache.org/docs/latest/ml-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T21:31:16.942833Z",
     "start_time": "2019-12-10T21:31:16.925378Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load -s task_2 assignment2.py\n",
    "def task_2(data_io, product_data):\n",
    "    # -----------------------------Column names--------------------------------\n",
    "    # Inputs:\n",
    "    salesRank_column = 'salesRank'\n",
    "    categories_column = 'categories'\n",
    "    asin_column = 'asin'\n",
    "    # Outputs:\n",
    "    category_column = 'category'\n",
    "    bestSalesCategory_column = 'bestSalesCategory'\n",
    "    bestSalesRank_column = 'bestSalesRank'\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Your implementation begins------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Put results in res dict --------------------------\n",
    "    res = {\n",
    "        'count_total': None,\n",
    "        'mean_bestSalesRank': None,\n",
    "        'variance_bestSalesRank': None,\n",
    "        'numNulls_category': None,\n",
    "        'countDistinct_category': None,\n",
    "        'numNulls_bestSalesCategory': None,\n",
    "        'countDistinct_bestSalesCategory': None\n",
    "    }\n",
    "    # Modify res:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----------------------------- Do not change -----------------------------\n",
    "    data_io.save(res, 'task_2')\n",
    "    return res\n",
    "    # -------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T22:19:19.308187Z",
     "start_time": "2019-12-09T22:19:04.274345Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "res = task_2(data_io, data_dict['product'][['asin', 'categories', 'salesRank']])\n",
    "print (res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Task 3: flattening ```related```\n",
    "\n",
    "Inside the ```related``` column there is a map containing four keys/attributes: ```also_bought```, ```also_viewed```, ```bought_together```, and ```buy_after_viewing```. Each of them contains an array of ```asin```s (Amazon product ID). We call these arrays attribute arrays. We need to flatten the schema by calculating the length of the arrays. In addition to the above, we would like to know the average prices of the products.\n",
    "\n",
    "1. The logic for all four attributes are identical. For the sake of simplicity, you are only required to flatten the ```also_viewed``` attribute. Your task is to implement the following function ```task_3()```.\n",
    "\n",
    "1. For each row, you need to :\n",
    "    1. First calculate the mean price of all products from the ```also_viewed``` attribute array. \n",
    "    1. Then you need to put the mean price in a new column, the name of which is ```meanPriceAlsoViewed```. When you calculate these mean values, remember to ignore the products if they do not match any record in ```product```, or if they have ```null``` in price.\n",
    "    1. Similary, put the length of that array in a new column ```countAlsoViewed```. You do not need to check if the product IDs in that array are dangling references or not. Put ```null``` (instead of zero) in the new column, if the attribute array is ```null``` or empty.\n",
    "\n",
    "1. You need to conduct the above operations, then extract some statistics out of the generated columns. You need to put the statistics in a python dictionary named ```res```. The description and schema of which is as follows:\n",
    "    ```\n",
    "    res\n",
    "     | -- count_total: int -- number of rows of the entire processed table\n",
    "     | -- mean_meanPriceAlsoViewed: float -- mean value of meanPriceAlsoViewed\n",
    "     | -- variance_meanPriceAlsoViewed: float -- variance of meanPriceAlsoViewed\n",
    "     | -- numNulls_meanPriceAlsoViewed: int -- count of null-value entries of meanPriceAlsoViewed\n",
    "     | -- mean_countAlsoViewed: float -- mean value of countAlsoViewed\n",
    "     | -- variance_countAlsoViewed: float -- variance of countAlsoViewed\n",
    "     | -- numNulls_countAlsoViewed: int -- count of null-value entries of countAlsoViewed\n",
    "    ```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T21:31:26.542481Z",
     "start_time": "2019-12-10T21:31:26.525050Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load -s task_3 assignment2.py\n",
    "def task_3(data_io, product_data):\n",
    "    # -----------------------------Column names--------------------------------\n",
    "    # Inputs:\n",
    "    asin_column = 'asin'\n",
    "    price_column = 'price'\n",
    "    attribute = 'also_viewed'\n",
    "    related_column = 'related'\n",
    "    # Outputs:\n",
    "    meanPriceAlsoViewed_column = 'meanPriceAlsoViewed'\n",
    "    countAlsoViewed_column = 'countAlsoViewed'\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Your implementation begins------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Put results in res dict --------------------------\n",
    "    res = {\n",
    "        'count_total': None,\n",
    "        'mean_meanPriceAlsoViewed': None,\n",
    "        'variance_meanPriceAlsoViewed': None,\n",
    "        'numNulls_meanPriceAlsoViewed': None,\n",
    "        'mean_countAlsoViewed': None,\n",
    "        'variance_countAlsoViewed': None,\n",
    "        'numNulls_countAlsoViewed': None\n",
    "    }\n",
    "    # Modify res:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----------------------------- Do not change -----------------------------\n",
    "    data_io.save(res, 'task_3')\n",
    "    return res\n",
    "    # -------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T22:20:41.442745Z",
     "start_time": "2019-12-09T22:19:19.358780Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "res = task_3(data_io, data_dict['product'][['asin', 'related', 'price']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Task 4: data imputation\n",
    "You may have noticed that there are lots of ```null``` values in the table. Now your task is to impute them with other values that can be used.\n",
    "\n",
    "Since we have already flattened the schema, we only have two types of values in our table: numerical (including integer and floating numbers) and string. Now you need to impute a numerical column ```price```, as well as a string column ```title```.\n",
    "\n",
    "1. Please implement a function ```task_4()```. For numerical column ```price```, first cast it to ```FloatType```. Then you want to impute the ```null``` values in the column with the mean value of all the not ```null``` values. Store the outputs in a new column ```meanImputedPrice```.\n",
    "1. Same as, but this time impute ```null``` values with the **median** value of all the not ```null``` values in column ```price```. Store the outputs in a new column ```medianImputedPrice```.\n",
    "1. As for the ```StringType``` columns, we want to simply impute with a special string ```'unknown'```. Please also impute empty strings ```''```. Store the outputs in a new column ```unknownImputedTitle```.\n",
    "\n",
    "1. You need to conduct the above operations, then extract some statistics out of the generated columns. You need to put the statistics in a python dictionary named ```res```. The description and schema are as follows:\n",
    "    ```\n",
    "    res\n",
    "     | -- count_total: int -- count of total rows of the entire table after above operations\n",
    "     | -- mean_meanImputedPrice: float or None -- mean\n",
    "     | -- variance_meanImputedPrice: float -- variance\n",
    "     | -- numNulls_meanImputedPrice: int -- count of null-value entries\n",
    "     | -- mean_medianImputedPrice: float or None -- mean\n",
    "     | -- variance_medianImputedPrice: float -- variance\n",
    "     | -- numNulls_medianImputedPrice: int -- count of null-value entries\n",
    "     | -- numUnknowns_unknownImputedTitle: float -- count of 'unknown' value entries\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T21:31:39.503390Z",
     "start_time": "2019-12-10T21:31:39.484724Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load -s task_4 assignment2.py\n",
    "def task_4(data_io, product_data):\n",
    "    # -----------------------------Column names--------------------------------\n",
    "    # Inputs:\n",
    "    price_column = 'price'\n",
    "    title_column = 'title'\n",
    "    # Outputs:\n",
    "    meanImputedPrice_column = 'meanImputedPrice'\n",
    "    medianImputedPrice_column = 'medianImputedPrice'\n",
    "    unknownImputedTitle_column = 'unknownImputedTitle'\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Your implementation begins------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Put results in res dict --------------------------\n",
    "    res = {\n",
    "        'count_total': None,\n",
    "        'mean_meanImputedPrice': None,\n",
    "        'variance_meanImputedPrice': None,\n",
    "        'numNulls_meanImputedPrice': None,\n",
    "        'mean_medianImputedPrice': None,\n",
    "        'variance_medianImputedPrice': None,\n",
    "        'numNulls_medianImputedPrice': None,\n",
    "        'numUnknowns_unknownImputedTitle': None\n",
    "    }\n",
    "    # Modify res:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----------------------------- Do not change -----------------------------\n",
    "    data_io.save(res, 'task_4')\n",
    "    return res\n",
    "    # -------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T22:20:47.953226Z",
     "start_time": "2019-12-09T22:20:41.523379Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "res = task_4(data_io, data_dict['product'][['price', 'title']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Task 5: embedding ```title``` with ```word2vec```\n",
    "*This task assumes the ```title``` column is already imputed with ```unknown```. We will provide the imputed data table ```product_processed_data```.*\n",
    "\n",
    "In this task we want to transform ```title``` into a fixed-length vector by training and then applying word2vec. \n",
    "\n",
    "1. You need to implement function ```task_5()```. For each row, \n",
    "    1. convert all characters in ```title``` to lowercase, \n",
    "    1. then split ```title``` by whitespace (```' '```) to an array of strings, store it in a new column ```titleArray```\n",
    "\n",
    "1. Train a word2vec model out of this column ```titleArray```, then for each row, transform ```titleArray``` into vectors. First transform every word in the array to vector, then simply averaging the vectors to obtain the vector for the title. Put the title vector in a new column named ```titleVector```. Do not try to implement word2vec yourself, instead, use ```M.feature.Word2Vec``` and it has built-it method to do the transformation. See instructions below.\n",
    "\n",
    "1. Use your trained word2vec model to get the 10 closest synonyms along with the similarity score (based on cosine similarity of word vectors, descending) each for three words inputed as ```<word_0>```, ```<word_1>```, and ```<word_2>```. ```M.feature.Word2Vec``` also has built-in method for this task.\n",
    "\n",
    "1. You need to conduct the above operations, then extract some statistics out of the generated columns. You need to put the statistics in a python dictionary named ```res```. The description and schema is as follows:\n",
    "    ```\n",
    "    res\n",
    "     | -- count_total: int -- count of total rows of the entire transformed table\n",
    "     | -- size_vocabulary: int -- the size of the vocabulary of your word2vec model\n",
    "     | -- word_0_synonyms: list -- synonyms tuples of word_0\n",
    "     |    | -- element: tuple -- tuple of format (synonym, score)\n",
    "     |    |    | -- element: string -- synonym\n",
    "     |    |    | -- element: float -- score\n",
    "     | -- word_1_synonyms: list \n",
    "     |    | -- element: tuple \n",
    "     |    |    | -- element: string \n",
    "     |    |    | -- element: float\n",
    "     | -- word_2_synonyms: list \n",
    "     |    | -- element: tuple \n",
    "     |    |    | -- element: string \n",
    "     |    |    | -- element: float\n",
    "    ```\n",
    "\n",
    "\n",
    "**word2vec instructions**:\n",
    "\n",
    "1. Set ```minCount```, the minimum number of times a token must appear to be included in the word2vec modelâ€™s vocabulary to be ```100```.\n",
    "\n",
    "1. Set the dimension of output word embedding to ```16```.\n",
    "\n",
    "1. You need to set the random seed as ```SEED```, this is a global variable defined to be 102.\n",
    "\n",
    "1. Set ```numPartitions``` to be ```4```.\n",
    "\n",
    "1. You should keep all other settings as default.\n",
    "\n",
    "1. ```M.feature.Word2Vec``` is not fully reproducible (although we have set the seed here). We are aware of the issue and your score will not be affected by its internal randomness.\n",
    "\n",
    "https://spark.apache.org/docs/latest/ml-features.html#word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T21:32:29.284661Z",
     "start_time": "2019-12-10T21:32:29.267237Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load -s task_5 assignment2.py\n",
    "def task_5(data_io, product_processed_data, word_0, word_1, word_2):\n",
    "    # -----------------------------Column names--------------------------------\n",
    "    # Inputs:\n",
    "    title_column = 'title'\n",
    "    # Outputs:\n",
    "    titleArray_column = 'titleArray'\n",
    "    titleVector_column = 'titleVector'\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Your implementation begins------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Put results in res dict --------------------------\n",
    "    res = {\n",
    "        'count_total': None,\n",
    "        'size_vocabulary': None,\n",
    "        'word_0_synonyms': [(None, None), ],\n",
    "        'word_1_synonyms': [(None, None), ],\n",
    "        'word_2_synonyms': [(None, None), ]\n",
    "    }\n",
    "    # Modify res:\n",
    "    res['count_total'] = product_processed_data_output.count()\n",
    "    res['size_vocabulary'] = model.getVectors().count()\n",
    "    for name, word in zip(\n",
    "        ['word_0_synonyms', 'word_1_synonyms', 'word_2_synonyms'],\n",
    "        [word_0, word_1, word_2]\n",
    "    ):\n",
    "        res[name] = model.findSynonymsArray(word, 10)\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----------------------------- Do not change -----------------------------\n",
    "    data_io.save(res, 'task_5')\n",
    "    return res\n",
    "    # -------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T22:26:05.015529Z",
     "start_time": "2019-12-09T22:20:47.999834Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "res = task_5(data_io, data_dict['product_processed'], 'piano', 'rice', 'laptop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Task 6: one-hot encoding ```category``` and PCA\n",
    "*Assume the schema of ```categories``` is already flattened and ```unknown``` imputed for the input data. We will provide you with the preprocessed table*\n",
    "\n",
    "Now you need to one-hot encode the categorical features. Also, these categories may be correlated and as a practice, we want to run PCA on these categories. \n",
    "    \n",
    "1. Implement function ```task_6()```. First one-hot encode ```category``` and put the output vectors in a new column ```categoryOneHot```. Note you do need to ensure the dimension of generated encoding vector equals to the size of domain. For example, if we have three categories in total: ```V = {'Electronics', 'Books', 'Appliances'}```. Then the encoding of 'Electronics' can be ```[1, 0, 0] or [0, 1, 0] or [0, 0, 1]``` but the dimension of this vector must be 3. Hint: before one-hot encoding a StringType column, you may need to first convert that column of strings to a column of numerical indices with ```M.feature.StringIndexer```. Then use ```M.feature.OneHotEncoderEstimator``` to do the encoding.\n",
    "\n",
    "1. Second, use ```M.feature.PCA``` on the transformed column. Reduce the dimension of each one-hot vector to ```15```, put the transformed vectors in a new column ```categoryPCA```. You can use ```M.feature.PCA``` for the task.\n",
    "\n",
    "1. Column  ```categoryOneHot``` and ```categoryPCA``` will be of VectorType. You do not need to worry about if the vectors are sparsely or densely represented.\n",
    "\n",
    "1. You need to conduct the above operations, then extract some statistics out of the generated columns. You need to put the statistics in a python dictionary named ```res```. The description and schema is as follows:\n",
    "    ```\n",
    "    res\n",
    "     | -- count_total: int -- count of total rows of the entire transformed table\n",
    "     | -- meanVector_categoryOneHot: list -- the mean vector of all transformed one-hot encoding vectors\n",
    "     |    | -- element: float -- each element of the mean vector, from first to last dimension\n",
    "     | -- meanVector_categoryPCA: list -- mean vector of PCA-transformed vectors\n",
    "     |    | -- element: float\n",
    "    ```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T21:32:39.991460Z",
     "start_time": "2019-12-10T21:32:39.974136Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load -s task_6 assignment2.py\n",
    "def task_6(data_io, product_processed_data):\n",
    "    # -----------------------------Column names--------------------------------\n",
    "    # Inputs:\n",
    "    category_column = 'category'\n",
    "    # Outputs:\n",
    "    categoryIndex_column = 'categoryIndex'\n",
    "    categoryOneHot_column = 'categoryOneHot'\n",
    "    categoryPCA_column = 'categoryPCA'\n",
    "    # -------------------------------------------------------------------------    \n",
    "\n",
    "    # ---------------------- Your implementation begins------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Put results in res dict --------------------------\n",
    "    res = {\n",
    "        'count_total': None,\n",
    "        'meanVector_categoryOneHot': [None, ],\n",
    "        'meanVector_categoryPCA': [None, ]\n",
    "    }\n",
    "    # Modify res:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----------------------------- Do not change -----------------------------\n",
    "    data_io.save(res, 'task_6')\n",
    "    return res\n",
    "    # -------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T22:29:57.717617Z",
     "start_time": "2019-12-09T22:29:51.132434Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "res = task_6(data_io, data_dict['product_processed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T21:23:18.882119Z",
     "start_time": "2019-11-26T21:23:18.873162Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "print (\"End to end time: {}\".format(time.time()-begin))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
